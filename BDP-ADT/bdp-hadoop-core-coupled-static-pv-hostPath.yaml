tosca_definitions_version: tosca_simple_yaml_1_0

imports:
  - https://raw.githubusercontent.com/micado-scale/tosca/v0.9.0/micado_types.yaml

repositories:
  docker_hub: https://hub.docker.com/

dsl_definitions:
  compute_properties: &compute_properties
    region_name: ADD_YOUR_REGION_NAME (e.g. us-east-2)
    image_id: ADD_YOUR_IMAGE_ID (e.g. ami-0d03add87774b12c5)
    instance_type: ADD_INSTANCE_TYPE (e.g. t2.xlarge)
    security_group_ids:
      - ADD_YOUR_SECURITY_GROUP_ID (e.g. sg-07d111ddf11da2d20)
    key_name: ADD_YOUR_KEY-NAME (e.g. your_ssh_key)

  endpoint_properties: &endpoint_properties
    interface_cloud: ADD_YOUR_ENDPOINT (e.g. ec2)
    root_block_device: {"volume_size": SPECIFY_VOLUME_SIZE-IN-GB (e.g. "80")}
    endpoint: ADD_YOUR_ENDPOINT (e.g. https://ec2.us-east-2.amazonaws.com)

topology_template:
  inputs:
    min_workers:
      type: integer
      description: Minimum workers and replicas for datanodes and nodemanagers
      required: yes

    max_workers:
      type: integer
      description: Maximum workers and replicas for datanodes and nodemanagers
      required: yes

  node_templates:

    hdfs-datanode-0-pv:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              kind: PersistentVolume
              apiVersion: v1
              metadata:
                name: hdfs-datanode-0-pv
                labels:
                  type: local
              spec:
                storageClassName: manual
                capacity:
                  storage: 20Gi
                accessModes:
                  - ReadWriteOnce
                hostPath:
                  path: "/datanode-0-data"

    hdfs-datanode-1-pv:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              kind: PersistentVolume
              apiVersion: v1
              metadata:
                name: hdfs-datanode-1-pv
                labels:
                  type: local
              spec:
                storageClassName: manual
                capacity:
                  storage: 20Gi
                accessModes:
                  - ReadWriteOnce
                hostPath:
                  path: "/datanode-1-data"

    hdfs-datanode-2-pv:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              kind: PersistentVolume
              apiVersion: v1
              metadata:
                name: hdfs-datanode-2-pv
                labels:
                  type: local
              spec:
                storageClassName: manual
                capacity:
                  storage: 20Gi
                accessModes:
                  - ReadWriteOnce
                hostPath:
                  path: "/datanode-2-data"


    hdfs-namenode:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: apps/v1
              kind: StatefulSet
              metadata:
                name: hdfs-namenode
              spec:
                serviceName: hdfs-namenode
                replicas: 1
                updateStrategy:
                  type: RollingUpdate
                podManagementPolicy: Parallel
                selector:
                  matchLabels:
                      app: hdfs
                      component: namenode
                template:
                  metadata:
                    labels:
                      app: hdfs
                      component: namenode
                  spec:
                    initContainers:
                      - name: hdfs-init
                        image: flokkr/hadoop
                        args: ["hadoop","version"]
                        env:
                          - name: "ENSURE_NAMENODE_DIR"
                            value: "/data/namenode"
                        envFrom:
                          - configMapRef:
                              name: hdfs-config
                        volumeMounts:
                          - name: "data"
                            mountPath: "/data"
                    containers:
                      - name: hdfs-namenode
                        image: flokkr/hadoop
                        args: ["hdfs","namenode"]
                        envFrom:
                          - configMapRef:
                              name: hdfs-config
                        volumeMounts:
                          - name: "data"
                            mountPath: "/data"
                    nodeSelector:
                      micado.eu/node_type: hdp-master
                    volumes:
                      - name: "data"
                        hostPath:
                          path: /namenode-data

    hdfs-config-configmap:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: v1
              kind: ConfigMap
              metadata:
                name: hdfs-config
              data:
                HDFS-SITE.XML_dfs.namenode.name.dir: "/data/namenode"
                HDFS-SITE.XML_dfs.datanode.data.dir: "/data/datanode"
                HDFS-SITE.XML_dfs.namenode.rpc-address: "hdfs-namenode-0.hdfs-namenode:9820"
                HDFS-SITE.XML_dfs.permissions: "false"
                HDFS-SITE.XML_hadoop.tmp.dir: "/tmp"
                LOG4J.PROPERTIES_log4j.rootLogger: "INFO, stdout"
                LOG4J.PROPERTIES_log4j.appender.stdout: "org.apache.log4j.ConsoleAppender"
                LOG4J.PROPERTIES_log4j.appender.stdout.layout: "org.apache.log4j.PatternLayout"
                LOG4J.PROPERTIES_log4j.appender.stdout.layout.ConversionPattern: "%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n"
                CORE-SITE.XML_fs.defaultFS: "hdfs://hdfs-namenode-0.hdfs-namenode:9820"
                CORE-SITE.XML_hadoop.tmp.dir: "/tmp"
                

    hdfs-namenode-public-service:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: v1
              kind: Service
              metadata:
                name: hdfs-namenode-public
              spec:
                ports:
                  - port: 9870
                    nodePort: 30010
                    name: web
                selector:
                  app: hdfs
                  component: namenode
                type: NodePort

    hdfs-namenode-service:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: v1
              kind: Service
              metadata:
                name: hdfs-namenode
              spec:
                clusterIP: None
                ports:
                  - port: 9870
                    name: web
                selector:
                  app: hdfs
                  component: namenode

    hdfs-datanode:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: apps/v1
              kind: StatefulSet
              metadata:
                name: hdfs-datanode
              spec:
                serviceName: hdfs-datanode
                replicas: {get_input: min_workers}
                updateStrategy:
                  type: RollingUpdate
                selector:
                  matchLabels:
                      app: hdfs
                      component: datanode
                podManagementPolicy: Parallel
                template:
                  metadata:
                    labels:
                      app: hdfs
                      component: datanode
                  spec:
                    affinity:
                      podAntiAffinity:
                        requiredDuringSchedulingIgnoredDuringExecution:
                        - labelSelector:
                            matchExpressions:
                              - key: "app"
                                operator: In
                                values:
                                - hdfs
                          topologyKey: "kubernetes.io/hostname"
                    containers:
                      - name: hdfs-datanode
                        image: flokkr/hadoop
                        args: ["hdfs","datanode"]
                        env:
                          - name: "WAITFOR"
                            value: "hdfs-namenode-0.hdfs-namenode:9820"
                        volumeMounts:
                        - name: data
                          mountPath: /data
                        envFrom:
                          - configMapRef:
                              name: hdfs-config
                    nodeSelector:
                      micado.eu/node_type: hdp-slave
                    volumes:
                      - name: config
                        configMap:
                          name: hdfs-config
                      #- name: "data"
                        #hostPath:
                          #path: /datanode-data
                volumeClaimTemplates:
                  - metadata:
                      name: data
                    spec:
                      accessModes: [ "ReadWriteOnce" ]
                      storageClassName: manual
                      resources:
                        requests:
                          storage: 20Gi
                      selector:
                        matchLabels:
                          type: local

    hdfs-datanode-service:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: v1
              kind: Service
              metadata:
                name: hdfs-datanode
              spec:
                ports:
                  - port: 9874
                    name: web
                clusterIP: None
                selector:
                  app: hdfs
                  component: datanode

    yarn-resourcemanager:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: apps/v1
              kind: StatefulSet
              metadata:
                name: yarn-resourcemanager
                labels:
                  app: yarn
                  component: resourcemanager
              spec:
                serviceName: yarn-resourcemanager
                replicas: 1
                updateStrategy:
                  type: RollingUpdate
                podManagementPolicy: Parallel
                selector:
                  matchLabels:
                      app: yarn
                      component: resourcemanager
                template:
                  metadata:
                    labels:
                      app: yarn
                      component: resourcemanager
                  spec:
                    containers:
                      - name: yarn-resourcemanager
                        image: flokkr/hadoop
                        args: ["yarn","resourcemanager"]
                        envFrom:
                          - configMapRef:
                              name: yarn-config
                          - configMapRef:
                              name: hdfs-config
                    nodeSelector:
                      micado.eu/node_type: hdp-master
                    volumes:
                      - name: config
                        configMap:
                          name: yarn-config
                      - name: "data"
                        hostPath:
                          path: /resourcemanager-data

    yarn-config-configmap:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: v1
              kind: ConfigMap
              metadata:
                name: yarn-config
              data:
                MAPRED-SITE.XML_mapreduce.framework.name: "yarn"
                MAPRED-SITE.XML_yarn.app.mapreduce.am.env: "HADOOP_MAPRED_HOME=/opt/hadoop"
                MAPRED-SITE.XML_mapreduce.map.env: "HADOOP_MAPRED_HOME=/opt/hadoop"
                MAPRED-SITE.XML_mapreduce.reduce.env: "HADOOP_MAPRED_HOME=/opt/hadoop"
                YARN-SITE.XML_yarn.resourcemanager.hostname: "yarn-resourcemanager-0.yarn-resourcemanager"
                YARN-SITE.XML_yarn.resourcemanager.bind-host: "0.0.0.0"
                YARN-SITE.XML_yarn.nodemanager.bind-host: "0.0.0.0"
                YARN-SITE.XML_yarn.webapp.ui2.enable: "true"
                YARN-SITE.XML_yarn.nodemanager.pmem-check-enabled: "false"
                YARN-SITE.XML_yarn.nodemanager.delete.debug-delay-sec: "600"
                YARN-SITE.XML_yarn.nodemanager.vmem-check-enabled: "false"
                YARN-SITE.XML_yarn.nodemanager.aux-services: "mapreduce_shuffle"
                YARN-SITE.XML_yarn.nodemanager.auxservices.mapreduce.shuffle.class: "org.apache.hadoop.mapred.ShuffleHandler"
                YARN_SITE.XML_yarn.timeline-service.hostname: yarn-timeline-0.yarn-timeline
                YARN_SITE.XML_yarn.log.server.url: http://yarn-timeline-0.yarn-timeline:8188/applicationhistory/logs/
                CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-applications: "10000"
                CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-am-resource-percent: "0.1"
                CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.resource-calculator: "org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator"
                CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.queues: "default"
                CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.capacity: "100"
                CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.user-limit-factor: "1"
                CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.maximum-capacity: "100"
                CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.state: "RUNNING"
                CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl_submit_applications: "*"
                CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl_administer_queue: "*"
                CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.node-locality-delay: "40"
                CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings: ""
                CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings-override.enable: "false"

    yarn-resourcemanager-public-service:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: v1
              kind: Service
              metadata:
                name: yarn-resourcemanager-public
              spec:
                ports:
                  - port: 8088
                    nodePort: 30040
                    name: web
                selector:
                  app: yarn
                  component: resourcemanager
                type: NodePort

    yarn-resourcemanager-service:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: v1
              kind: Service
              metadata:
                name: yarn-resourcemanager
              spec:
                ports:
                  - port: 8088
                    name: web
                clusterIP: None
                selector:
                  app: yarn
                  component: resourcemanager

    yarn-nodemanager:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: apps/v1
              kind: StatefulSet
              metadata:
                name: yarn-nodemanager
                labels:
                  app: yarn
                  component: nodemanager
              spec:
                serviceName: yarn-nodemanager
                replicas: {get_input: min_workers}
                updateStrategy:
                  type: RollingUpdate
                podManagementPolicy: Parallel
                selector:
                  matchLabels:
                      app: yarn
                      component: nodemanager
                template:
                  metadata:
                    labels:
                      app: yarn
                      component: nodemanager
                  spec:
                    affinity:
                      podAntiAffinity:
                        requiredDuringSchedulingIgnoredDuringExecution:
                        - labelSelector:
                            matchExpressions:
                              - key: "app"
                                operator: In
                                values:
                                - yarn
                          topologyKey: "kubernetes.io/hostname"
                    containers:
                      - name: yarn-nodemanager
                        image: flokkr/hadoop
                        args: ["yarn","nodemanager"]
                        env:
                            - name: WAITFOR
                              value: "yarn-resourcemanager-0.yarn-resourcemanager:8031"
                        envFrom:
                          - configMapRef:
                              name: yarn-config
                          - configMapRef:
                              name: hdfs-config
                        volumeMounts:
                          - name: "data"
                            mountPath: "/data"
                    nodeSelector:
                      micado.eu/node_type: hdp-slave
                    volumes:
                      - name: "data"
                        hostPath:
                          path: /nodemanager-data

    yarn-nodemanager-public-service:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: v1
              kind: Service
              metadata:
                name: yarn-nodemanager-public
              spec:
                ports:
                  - port: 8042
                    nodePort: 30030
                    name: web
                selector:
                  app: yarn
                  component: nodemanager
                type: NodePort

    yarn-nodemanager-service:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: v1
              kind: Service
              metadata:
                name: yarn-nodemanager
              spec:
                clusterIP: None
                ports:
                  - port: 8042
                    name: web
                selector:
                  app: yarn
                  component: nodemanager

                
    hdp-master:
      type: tosca.nodes.MiCADO.EC2.Compute
      properties: *compute_properties
      interfaces:
        Terraform:
          create:
            inputs: *endpoint_properties

    hdp-slave:
      type: tosca.nodes.MiCADO.EC2.Compute
      properties: *compute_properties
      interfaces:
        Terraform:
          create:
            inputs: *endpoint_properties
      capabilities:
        scalable:
          properties:
            min_instances: {get_input: min_workers}
            max_instances: {get_input: max_workers}

  policies:
    - monitoring:
        type: tosca.policies.Monitoring.MiCADO
        properties:
          enable_container_metrics: true
          enable_node_metrics: true
    - scalability:
        type: tosca.policies.Scaling.MiCADO.VirtualMachine.CPU.datanode
        targets: [ hdp-slave ]
        properties:
          constants:
            NODE_NAME: 'hdp-slave'
            NODE_TH_MAX: '50'
            NODE_TH_MIN: '10'
          min_instances: {get_input: min_workers}
          max_instances: {get_input: max_workers}
    - scalability:
        type: tosca.policies.Scaling.MiCADO.Container.CPU.datanode
        targets: [ hdfs-datanode ]
        properties:
          constants:
            SERVICE_NAME: 'hdfs-datanode'
            SERVICE_FULL_NAME: 'hdfs-datanode'
            SERVICE_TH_MAX: '50'
            SERVICE_TH_MIN: '10'
          min_instances: {get_input: min_workers}
          max_instances: {get_input: max_workers}
    - scalability:
        type: tosca.policies.Scaling.MiCADO.Container.CPU.nodemanager
        targets: [ yarn-nodemanager ]
        properties:
          constants:
            SERVICE_NAME: 'yarn-nodemanager'
            SERVICE_FULL_NAME: 'yarn-nodemanager'
            SERVICE_TH_MAX: '50'
            SERVICE_TH_MIN: '10'
          min_instances: {get_input: min_workers}
          max_instances: {get_input: max_workers}

policy_types:
  tosca.policies.Scaling.MiCADO.Container.CPU.datanode:
    derived_from: tosca.policies.Scaling.MiCADO
    description: base MiCADO policy defining data sources, constants, queries, alerts, limits and rules
    properties:
      alerts:
        type: list
        description: pre-define alerts for container CPU
        default:
        - alert: service_overloaded
          expr: 'avg(rate(container_cpu_usage_seconds_total{container_label_io_kubernetes_container_name="{{SERVICE_FULL_NAME}}"}[60s]))*100 > {{SERVICE_TH_MAX}}'
          for: 30s
        - alert: service_underloaded
          expr: 'avg(rate(container_cpu_usage_seconds_total{container_label_io_kubernetes_container_name="{{SERVICE_FULL_NAME}}"}[60s]))*100 < {{SERVICE_TH_MIN}}'
          for: 30s
        required: true
      scaling_rule:
        type: string
        description: pre-define scaling rule for container CPU
        default: |
          if len(m_nodes) == m_node_count:
            if service_overloaded or m_node_count > m_container_count:
            #if service_overloaded:
              m_container_count+=1
            if service_underloaded:
              m_container_count-=1
          else:
            print('Transient phase, skipping update of containers...')
        required: true

  tosca.policies.Scaling.MiCADO.Container.CPU.nodemanager:
    derived_from: tosca.policies.Scaling.MiCADO
    description: base MiCADO policy defining data sources, constants, queries, alerts, limits and rules
    properties:
      alerts:
        type: list
        description: pre-define alerts for container CPU
        default:
        - alert: service_overloaded
          expr: 'avg(rate(container_cpu_usage_seconds_total{container_label_io_kubernetes_container_name="{{SERVICE_FULL_NAME}}"}[60s]))*100 > {{SERVICE_TH_MAX}}'
          for: 30s
        - alert: service_underloaded
          expr: 'avg(rate(container_cpu_usage_seconds_total{container_label_io_kubernetes_container_name="{{SERVICE_FULL_NAME}}"}[60s]))*100 < {{SERVICE_TH_MIN}}'
          for: 30s
        required: true
      scaling_rule:
        type: string
        description: pre-define scaling rule for container CPU
        default: |
          if len(m_nodes) == m_node_count:
            if service_overloaded or m_node_count > m_container_count:
            #if service_overloaded:
              m_container_count+=1
            if service_underloaded:
              m_container_count-=1
          else:
            print('Transient phase, skipping update of containers...')
        required: true

  tosca.policies.Scaling.MiCADO.VirtualMachine.CPU.datanode:
    derived_from: tosca.policies.Scaling.MiCADO
    description: base MiCADO policy defining data sources, constants, queries, alerts, limits and rules
    properties:
      alerts:
        type: list
        description: pre-define alerts for VM CPU
        default:
        - alert: node_overloaded
          expr: '(100-(avg(rate(node_cpu_seconds_total{node="{{ NODE_NAME }}", mode="idle"}[60s]))*100)) > {{NODE_TH_MAX}}'
          for: 1m
        - alert: node_underloaded
          expr: '(100-(avg(rate(node_cpu_seconds_total{node="{{ NODE_NAME }}", mode="idle"}[60s]))*100)) < {{NODE_TH_MIN}}'
          for: 1m
        required: true
      scaling_rule:
        type: string
        description: pre-define scaling rule for VM CPU
        default: |
          if len(m_nodes) <= m_node_count and m_time_since_node_count_changed > 60:
            if node_overloaded:
              m_node_count+=1
            if node_underloaded:
              m_node_count-=1
          else:
            print('Transient phase, skipping update of nodes...')
        required: true
